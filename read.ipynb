{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "grandfather = open('stimuli_grandfather.txt', 'r')\n",
    "rainbow = open('stimuli_rainbow.txt', 'r')\n",
    "windsun = open('stimuli_windsun.txt', 'r')\n",
    "\n",
    "grandfather_lines = grandfather.read().split('\\n')\n",
    "rainbow_lines = rainbow.read().split('\\n')\n",
    "windsun_lines = windsun.read().split('\\n')\n",
    "\n",
    "lines = []\n",
    "lines.append((grandfather_lines, 'grandfather'))\n",
    "lines.append((rainbow_lines, 'rainbow'))\n",
    "lines.append((windsun_lines, 'windsun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat lines 3 times\n",
    "lines = [item for sublist in lines for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(lines, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, header = pyxdf.load_xdf('sub-060095_task-sentances_run-001.xdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_stream = data[0]\n",
    "markers_stream = data[0]\n",
    "audio_stream = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_buffer = audio_stream['time_series'].squeeze()\n",
    "audio_fs = 44100\n",
    "audio_channels = int(audio_stream['info']['channel_count'][0])\n",
    "\n",
    "# convert audio_data_buffer to wav \n",
    "\n",
    "sf.write('audio.wav', audio_data_buffer, audio_fs, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('test.wav',data[0]['time_series'],44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XDF file\n",
    "data, _ = pyxdf.load_xdf('data.xdf')\n",
    "\n",
    "# Find the audio and EEG streams\n",
    "audio_stream = None\n",
    "eeg_stream = None\n",
    "audio_stream = data[2]\n",
    "markers_stream = data[1]\n",
    "eeg_stream = data[0]\n",
    "\n",
    "# Check that both streams were found\n",
    "if audio_stream is None or eeg_stream is None:\n",
    "    raise Exception('Could not find both streams')\n",
    "\n",
    "# Align the streams based on their timestamps\n",
    "audio_data = audio_stream['time_series']\n",
    "audio_timestamps = audio_stream['time_stamps']\n",
    "eeg_data = eeg_stream['time_series']\n",
    "eeg_timestamps = eeg_stream['time_stamps']\n",
    "marker_data = markers_stream['time_series']\n",
    "marker_timestamps = markers_stream['time_stamps']\n",
    "\n",
    "# Now you can align the data based on the timestamps\n",
    "# This is a simple example that just finds the closest EEG sample for each audio sample\n",
    "aligned_data = []\n",
    "for i in range(len(audio_data)):\n",
    "    audio_sample = audio_data[i]\n",
    "    audio_timestamp = audio_timestamps[i]\n",
    "\n",
    "    # Find the closest EEG sample\n",
    "    closest_eeg_index = (np.abs(eeg_timestamps - audio_timestamp)).argmin()\n",
    "    closest_eeg_sample = eeg_data[closest_eeg_index]\n",
    "    \n",
    "    # Add the aligned samples to the result\n",
    "    aligned_data.append((audio_sample, closest_eeg_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated audio and eeg data into numpy arrays\n",
    "audio_data = np.array([x[0] for x in aligned_data])\n",
    "eeg_data = np.array([x[1] for x in aligned_data])\n",
    "print(audio_data.shape, eeg_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save audio_data as .wav file\n",
    "import soundfile as sf\n",
    "sf.write('audio_data.wav', audio_data, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in data:\n",
    "    print(stream['info']['name'][0])\n",
    "    print(stream['time_series'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
